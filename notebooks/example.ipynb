{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the path to the parent directory to sys\n",
    "import sys, os\n",
    "\n",
    "# If current directory is called 'notebooks', chdir to the parent\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "    \n",
    "sys.path.append('attribution')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from constants import ModelNames\n",
    "from model_utils import Model\n",
    "from dataset_utils import GSM8kDataset, PaddingCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Dev Projects\\RUG MSC AI 2024\\Advanced Topics in NLP\\IKNLP-Attribution\n"
     ]
    }
   ],
   "source": [
    "# print pwd\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "model = Model(ModelNames.QwenInstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 7473\n"
     ]
    }
   ],
   "source": [
    "# Create a training dataset\n",
    "train_dataset = GSM8kDataset(model.tokenizer, split='train')\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n"
     ]
    }
   ],
   "source": [
    "# Get a single example\n",
    "sample = train_dataset[0]\n",
    "print(f\"Question: {sample['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "\n",
      "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "Answer: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "#### 10\n",
      "\n",
      "Question: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "Answer: In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\n",
      "Betty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\n",
      "This means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\n",
      "#### 5\n",
      "\n",
      "Question: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n",
      "Answer: Maila read 12 x 2 = <<12*2=24>>24 pages today.\n",
      "So she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\n",
      "There are 120 - 36 = <<120-36=84>>84 pages left to be read.\n",
      "Since she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\n",
      "#### 42\n",
      "\n",
      "Question: James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
      "Answer: He writes each friend 3*2=<<3*2=6>>6 pages a week\n",
      "So he writes 6*2=<<6*2=12>>12 pages every week\n",
      "That means he writes 12*52=<<12*52=624>>624 pages a year\n",
      "#### 624\n"
     ]
    }
   ],
   "source": [
    "# View the generated instructions\n",
    "print(train_dataset.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 1319\n"
     ]
    }
   ],
   "source": [
    "# Create a test dataset using the same instructions\n",
    "test_dataset = GSM8kDataset(model.tokenizer, instructions=train_dataset.instructions, split='test')\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating batches: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "Generating batches: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Generating batches: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Generating batches: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "Generating batches: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time over 5 runs: 2.6326 seconds\n",
      "Number of tokens in output: 59\n",
      "Tokens per second: 22.41\n",
      "['Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batch processing with padding collator\n",
    "padding_collator = PaddingCollator(model.tokenizer)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "test_batch_size = 1\n",
    "sample = train_dataset[0]\n",
    "num_runs = 5\n",
    "\n",
    "sample_loader = DataLoader([sample], batch_size=1, collate_fn=padding_collator)\n",
    "\n",
    "# Run multiple times to get average performance\n",
    "times = []\n",
    "for _ in range(num_runs):\n",
    "    start = time.time()\n",
    "    output = model.generate_responses(sample_loader)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "# Get final output and token count\n",
    "num_tokens = len(model.tokenizer.tokenize(output[0]))\n",
    "mean_time = np.mean(times)\n",
    "tokens_per_second = num_tokens / mean_time\n",
    "\n",
    "print(f\"Mean inference time over {num_runs} runs: {mean_time:.4f} seconds\")\n",
    "print(f\"Number of tokens in output: {num_tokens}\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches:   0%|          | 0/42 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating batches: 100%|██████████| 42/42 [19:49<00:00, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Janet's ducks lay 16 eggs per day.\\nShe eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\\nShe bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left.\\nShe sells the remaining eggs at $2 per egg, so she makes 9 * $2 = $18 every day.\\nThe answer is $\\\\boxed{18}$.\", ' It takes 2 bolts of blue fiber.\\nAnd it takes half that amount of white fiber, so it takes 2/2 = 1 bolt of white fiber.\\nTo find the total number of bolts needed, we add the number of bolts of blue fiber and the number of bolts of white fiber together.\\nSo, the total number of bolts is 2 (blue) + 1 (white) = 3 bolts.\\nThe answer is $\\\\boxed{3}$.', ' The value of the house after repairs is $80,000 + $50,000 = $130,000.\\nThe increase in value is 150% of the original price, so it is 150/100 * $80,000 = $120,000.\\nAdding this increase to the original price gives us a final price of $80,000 + $120,000 = $200,000.\\nTo find the profit, we subtract the cost from the final price, so the profit is $200,000 - $80,000 = $120,000.\\nThe answer is $\\\\boxed{120000}$.', 'He runs 60*3=<<60*3=180>>180 meters a week\\nHe runs 180*3=<<180*3=540>>540 meters a week\\n#### 540', 'She needs 15 + 25 = 40 cups of feed for all of the chickens.\\nShe needs to divide this amount by the number of chickens so 40 / 20 = 2 cups of feed for the last meal.\\n#### 2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Be careful with the batch size\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=padding_collator)\n",
    "\n",
    "# Generate responses using the model\n",
    "generations = model.generate_responses(test_dataloader)\n",
    "print(generations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generations to a CSV file\n",
    "df = pd.DataFrame(generations, columns=['response'])\n",
    "df.to_csv('results/gsm8k_generations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generations from the CSV file\n",
    "df = pd.read_csv('results/gsm8k_generations.csv')\n",
    "generations = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8k Accuracy: 0.5739 (757/1319)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Generated answer:  Janet's ducks lay 16 eggs per day.\n",
      "She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\n",
      "She bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left.\n",
      "She sells the remaining eggs at $2 per egg, so she makes 9 * $2 = $18 every day.\n",
      "The answer is $\\boxed{18}$.\n",
      "Extracted generated answer: 18.0\n",
      "Extracted ground truth: 18.0\n",
      "\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "Generated answer:  It takes 2 bolts of blue fiber.\n",
      "And it takes half that amount of white fiber, so it takes 2/2 = 1 bolt of white fiber.\n",
      "To find the total number of bolts needed, we add the number of bolts of blue fiber and the number of bolts of white fiber together.\n",
      "So, the total number of bolts is 2 (blue) + 1 (white) = 3 bolts.\n",
      "The answer is $\\boxed{3}$.\n",
      "Extracted generated answer: 3.0\n",
      "Extracted ground truth: 3.0\n",
      "\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "Generated answer:  The value of the house after repairs is $80,000 + $50,000 = $130,000.\n",
      "The increase in value is 150% of the original price, so it is 150/100 * $80,000 = $120,000.\n",
      "Adding this increase to the original price gives us a final price of $80,000 + $120,000 = $200,000.\n",
      "To find the profit, we subtract the cost from the final price, so the profit is $200,000 - $80,000 = $120,000.\n",
      "The answer is $\\boxed{120000}$.\n",
      "Extracted generated answer: 120000.0\n",
      "Extracted ground truth: 70000.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy on GSM8k\n",
    "from dataset_utils import is_correct_gsm8k, extract_answer_gsm8k\n",
    "\n",
    "# Get ground truth answers\n",
    "gt_answers = [sample['answer'] for sample in test_dataset]\n",
    "\n",
    "# Calculate correct predictions\n",
    "correct = 0\n",
    "for pred, gt in zip(generations, gt_answers):\n",
    "    if is_correct_gsm8k(pred, {'answer': gt}):\n",
    "        correct += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(test_dataset)\n",
    "print(f\"GSM8k Accuracy: {accuracy:.4f} ({correct}/{len(test_dataset)})\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(3):  # Show first 3 examples\n",
    "    print(f\"\\nQuestion: {test_dataset[i]['question']}\")\n",
    "    print(f\"Generated answer: {generations[i]}\")\n",
    "    print(f\"Extracted generated answer: {extract_answer_gsm8k(generations[i])}\")\n",
    "    print(f\"Extracted ground truth: {extract_answer_gsm8k(test_dataset[i]['answer'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
