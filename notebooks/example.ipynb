{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Requirement already satisfied: packaging in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from datasets) (24.0)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting requests>=2.32.2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pandas in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.2.0-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: colorama in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: propcache, multidict, idna, frozenlist, yarl, urllib3, charset-normalizer, certifi, attrs, async-timeout, aiosignal, aiohappyeyeballs, tqdm, requests, pyyaml, fsspec, filelock, dill, aiohttp, xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.4.1 dill-0.3.8 filelock-3.18.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 idna-3.10 multidict-6.2.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'E:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'E:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "Requirement already satisfied: requests in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: colorama in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: tokenizers, safetensors, regex, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'E:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Collecting torch>=2.0.0\n",
      "  Downloading torch-2.6.0-cp310-cp310-win_amd64.whl (204.2 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: psutil in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.0)\n",
      "Requirement already satisfied: filelock in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: requests in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: colorama in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, torch, accelerate\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.5.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'E:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'E:\\kaggle_comps\\predict_future_sales\\code\\venv\\new-venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install pandas\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the path to the parent directory to sys\n",
    "import sys, os\n",
    "\n",
    "# If current directory is called 'notebooks', chdir to the parent\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "    \n",
    "sys.path.append('attribution')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from constants import ModelNames, DatasetNames, LANGUAGE_MAPPING\n",
    "from model_utils import Model \n",
    "from dataset_utils import GSMDataset, PaddingCollator, is_correct_gsm, extract_answer_gsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Masters\\Rug\\Courses\\2a\\Advance NLP\\Project\\code\\IKNLP-Attribution\n"
     ]
    }
   ],
   "source": [
    "# print pwd\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "model = Model(ModelNames.QwenInstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 8\n"
     ]
    }
   ],
   "source": [
    "# Create a training dataset\n",
    "train_dataset = GSMDataset(DatasetNames.MGSM, model.tokenizer, config='en')\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n"
     ]
    }
   ],
   "source": [
    "# Get a single example\n",
    "sample = train_dataset[0]\n",
    "print(f\"{sample['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Step-by-Step Answer:\n",
      "- Roger started with 5 balls.\n",
      "- 2 cans of 3 tennis balls each is 6 tennis balls.\n",
      "- 5 + 6 = 11.\n",
      "The answer is 11.\n",
      "\n",
      "Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
      "Step-by-Step Answer:\n",
      "- There are 4 days from monday to thursday.\n",
      "- 5 computers were added each day.\n",
      "- That means in total 4 * 5 = 20 computers were added.\n",
      "- There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
      "The answer is 29.\n",
      "\n",
      "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
      "Step-by-Step Answer:\n",
      "- Leah had 32 chocolates and Leah’s sister had 42.\n",
      "- That means there were originally 32 + 42 = 74 chocolates.\n",
      "- 35 have been eaten.\n",
      "- So in total they still have 74 - 35 = 39 chocolates.\n",
      "The answer is 39.\n",
      "\n",
      "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
      "Step-by-Step Answer:\n",
      "- He has 5 toys.\n",
      "- He got 2 from mom, so after that he has 5 + 2 = 7 toys.\n",
      "- Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys.\n",
      "The answer is 9.\n",
      "\n",
      "Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
      "Step-by-Step Answer:\n",
      "- Michael started with 58 golf balls and lost 23, so he has 58 - 23 = 35.\n",
      "- After he lost 2 more, he has 35 - 2 = 33 balls now.\n",
      "The answer is 33.\n"
     ]
    }
   ],
   "source": [
    "# View the generated instructions\n",
    "print(train_dataset.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 250\n"
     ]
    }
   ],
   "source": [
    "# Create a test dataset using the same instructions\n",
    "test_dataset = GSMDataset(DatasetNames.MGSM, model.tokenizer, instructions=train_dataset.instructions, split='test', config='en')\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time over 1 runs: 7.6519 seconds\n",
      "Number of tokens in output: 79\n",
      "Tokens per second: 10.32\n",
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "['Step-by-step answer:\\n- Roger starts with 5 tennis balls.\\n- He buys 2 more cans of tennis balls, each containing 3 tennis balls.\\n- Therefore, he gets an additional \\\\(2 \\\\times 3 = 6\\\\) tennis balls.\\n- Now, Roger has \\\\(5 + 6 = 11\\\\) tennis balls in total.\\nThe answer is 11.<']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batch processing with padding collator\n",
    "padding_collator = PaddingCollator(model.tokenizer)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sample = train_dataset[0]\n",
    "num_runs = 1\n",
    "\n",
    "sample_loader = DataLoader([sample], batch_size=1, collate_fn=padding_collator)\n",
    "\n",
    "# Run multiple times to get average performance\n",
    "times = []\n",
    "for _ in range(num_runs):\n",
    "    start = time.time()\n",
    "    output = model.generate_responses(sample_loader)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "# Get final output and token count\n",
    "num_tokens = len(model.tokenizer.tokenize(output[0]))\n",
    "mean_time = np.mean(times)\n",
    "tokens_per_second = num_tokens / mean_time\n",
    "\n",
    "print(f\"Mean inference time over {num_runs} runs: {mean_time:.4f} seconds\")\n",
    "print(f\"Number of tokens in output: {num_tokens}\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n",
    "print(sample['question'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches:   0%|          | 0/42 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating batches: 100%|██████████| 42/42 [19:49<00:00, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Janet's ducks lay 16 eggs per day.\\nShe eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\\nShe bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left.\\nShe sells the remaining eggs at $2 per egg, so she makes 9 * $2 = $18 every day.\\nThe answer is $\\\\boxed{18}$.\", ' It takes 2 bolts of blue fiber.\\nAnd it takes half that amount of white fiber, so it takes 2/2 = 1 bolt of white fiber.\\nTo find the total number of bolts needed, we add the number of bolts of blue fiber and the number of bolts of white fiber together.\\nSo, the total number of bolts is 2 (blue) + 1 (white) = 3 bolts.\\nThe answer is $\\\\boxed{3}$.', ' The value of the house after repairs is $80,000 + $50,000 = $130,000.\\nThe increase in value is 150% of the original price, so it is 150/100 * $80,000 = $120,000.\\nAdding this increase to the original price gives us a final price of $80,000 + $120,000 = $200,000.\\nTo find the profit, we subtract the cost from the final price, so the profit is $200,000 - $80,000 = $120,000.\\nThe answer is $\\\\boxed{120000}$.', 'He runs 60*3=<<60*3=180>>180 meters a week\\nHe runs 180*3=<<180*3=540>>540 meters a week\\n#### 540', 'She needs 15 + 25 = 40 cups of feed for all of the chickens.\\nShe needs to divide this amount by the number of chickens so 40 / 20 = 2 cups of feed for the last meal.\\n#### 2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Be careful with the batch size\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=padding_collator)\n",
    "\n",
    "# Generate responses using the model\n",
    "generations = model.generate_responses(test_dataloader)\n",
    "print(generations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generations to a CSV file\n",
    "df = pd.DataFrame(generations, columns=['response'])\n",
    "df.to_csv('results/gsm8k_generations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generations from the CSV file\n",
    "df = pd.read_csv('results/gsm8k_generations.csv')\n",
    "generations = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8k Accuracy: 0.5739 (757/1319)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Generated answer:  Janet's ducks lay 16 eggs per day.\n",
      "She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\n",
      "She bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left.\n",
      "She sells the remaining eggs at $2 per egg, so she makes 9 * $2 = $18 every day.\n",
      "The answer is $\\boxed{18}$.\n",
      "Extracted generated answer: 18.0\n",
      "Extracted ground truth: 18.0\n",
      "\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "Generated answer:  It takes 2 bolts of blue fiber.\n",
      "And it takes half that amount of white fiber, so it takes 2/2 = 1 bolt of white fiber.\n",
      "To find the total number of bolts needed, we add the number of bolts of blue fiber and the number of bolts of white fiber together.\n",
      "So, the total number of bolts is 2 (blue) + 1 (white) = 3 bolts.\n",
      "The answer is $\\boxed{3}$.\n",
      "Extracted generated answer: 3.0\n",
      "Extracted ground truth: 3.0\n",
      "\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "Generated answer:  The value of the house after repairs is $80,000 + $50,000 = $130,000.\n",
      "The increase in value is 150% of the original price, so it is 150/100 * $80,000 = $120,000.\n",
      "Adding this increase to the original price gives us a final price of $80,000 + $120,000 = $200,000.\n",
      "To find the profit, we subtract the cost from the final price, so the profit is $200,000 - $80,000 = $120,000.\n",
      "The answer is $\\boxed{120000}$.\n",
      "Extracted generated answer: 120000.0\n",
      "Extracted ground truth: 70000.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy on GSM8k\n",
    "from dataset_utils import is_correct_gsm, extract_answer_gsm\n",
    "\n",
    "# Get ground truth answers\n",
    "gt_answers = [sample['answer'] for sample in test_dataset]\n",
    "\n",
    "# Calculate correct predictions\n",
    "correct = 0\n",
    "for pred, gt in zip(generations, gt_answers):\n",
    "    if is_correct_gsm(pred, gt):\n",
    "        correct += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(test_dataset)\n",
    "print(f\"GSM8k Accuracy: {accuracy:.4f} ({correct}/{len(test_dataset)})\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(3):  # Show first 3 examples\n",
    "    print(f\"\\nQuestion: {test_dataset[i]['question']}\")\n",
    "    print(f\"Generated answer: {generations[i]}\")\n",
    "    print(f\"Extracted generated answer: {extract_answer_gsm(generations[i])}\")\n",
    "    print(f\"Extracted ground truth: {extract_answer_gsm(test_dataset[i]['answer'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GSMDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load MGSM\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mgsm_test \u001b[38;5;241m=\u001b[39m \u001b[43mGSMDataset\u001b[49m(DatasetNames\u001b[38;5;241m.\u001b[39mMGSM, model\u001b[38;5;241m.\u001b[39mtokenizer, instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load generations from mgsm_en_Qwen2-1.5B-Instruct_results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/mgsm_en_Qwen2-1.5B-Instruct_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GSMDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load MGSM\n",
    "mgsm_test = GSMDataset(DatasetNames.MGSM, model.tokenizer, instructions='', split='test', config='en')\n",
    "\n",
    "# Load generations from mgsm_en_Qwen2-1.5B-Instruct_results\n",
    "df = pd.read_csv('results/mgsm_en_Qwen2-1.5B-Instruct_results.csv')\n",
    "mgsm_generations = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGSM Accuracy: 0.5760 (144/250)\n"
     ]
    }
   ],
   "source": [
    "# Get ground truth answers\n",
    "mgsm_gt_answers = [sample for sample in mgsm_test.dataset['answer_number']]\n",
    "\n",
    "# Calculate correct predictions\n",
    "correct = 0\n",
    "for pred, gt in zip(mgsm_generations, mgsm_gt_answers):\n",
    "    if extract_answer_gsm(pred) == gt:\n",
    "        correct += 1\n",
    "        \n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(mgsm_test)\n",
    "print(f\"MGSM Accuracy: {accuracy:.4f} ({correct}/{len(mgsm_test)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ধাপে ধাপে উত্তর: সোমবার থেকে বৃহস্পতিবার 4দিন হয়। প্রতিদিন 5টি করে কম্পিউটার যোগ করা হয়েছে। যার অর্থ মোট 4 * 5 = 20টি কম্পিউটার যোগ করা হয়েছে। শুরুতে 9টি কম্পিউটার ছিল, তাই এখন 9 + 20 = 29টি কম্পিউটার রয়েছে। উত্তর হল 29।\n",
      "Schritt-für-Schritt-Antwort: Michael hatte anfangs 58 Golfbälle und hat 23 verloren, sodass er 58 - 23 = 35 hat. Nachdem er 2 weitere verloren hat, hat er jetzt 35 - 2 = 33 Bälle. Die Antwort lautet 33.\n",
      "Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "Respuesta paso a paso: Tiene 5 juguetes. Recibió 2 de la mamá, por lo que después de eso tiene 5 + 2 = 7 juguetes. Luego, recibió 2 más del papá, así que en total tiene 7 + 2 = 9 juguetes. La respuesta es 9.\n",
      "Réponse étape par étape : 5 bagels à 3 $ chacun coûtent 5 x 3 = 15 dollars. Olivia avait 23 dollars au départ, il lui reste donc 23 - 15 = 8 dollars. La réponse est 8.\n",
      "ステップごとの答え：月曜から木曜まで4日あります。毎日5台のコンピューターが追加されます。つまり、全部で4*5=20台のコンピューターが追加されました。最初に9台のコンピューターがあったので、今は9+20=29台のコンピューターとなります。答えは29です。\n",
      "Пошаговое решение: в начале у Роджера было 5 мячей. 2 банки по 3 теннисных мяча каждая — это 6 теннисных мячей. 5 + 6 = 11. Ответ — 11.\n",
      "Jibu la Hatua kwa Hatua: Lea alikuwa na chokoleti 32 na dadake Leah alikuwa na 42. Hiyo inamaanisha kuwa awali kulikuwa na chokoleti 32 + 42 + 74. 35 zimeliwa. Hivyo kwa jumla bado kuna chokoleti 74 - 35 = 39. Jibu ni 39.\n",
      "దశలవారీగా సమాధానం: సోమవారం నుంచి గురువారం వరకు 4 రోజులున్నాయి. ప్రతిరోజూ 5 కంప్యూటర్‌లు జోడించబడ్డాయి. అంటే మొత్తం 4*5=20 కంప్యూటర్‌లు జోడించబడ్డాయి. ప్రారంభంలో 9 కంప్యూటర్‌లు ఉన్నాయి, అందువల్ల ఇప్పుడు అవి 9+20=29 కంప్యూటర్‌లు సమాధానం 29.\n",
      "คำตอบทีละขั้นตอน: ลีอามีช็อกโกแลตอยู่ 32 ชิ้น และน้องสาวมีช็อกโกแลตอยู่ 42 ชิ้น แสดงว่าเดิมมีช็อกโกแลตอยู่ 32 + 42 = 74 ชิ้น หากทานไปแล้ว 35 ชิ้น ดังนั้นจะเหลือช็อกโกแลตทั้งหมดอยู่ 74 - 35 = 39 ชิ้น คำตอบคือ 39\n",
      "逐步解答：利亚有 32 块巧克力，利亚的妹妹有 42 块。这意味着原来有 32 + 42 = 74 块巧克力。35 块被吃掉了。所以她们一共还有 74 - 35 = 39 块巧克力。答案是 39。\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "\n",
    "answers = {}\n",
    "\n",
    "for key in LANGUAGE_MAPPING.keys():\n",
    "    mgsm_multilingual = datasets.load_dataset('juletxara/mgsm', key)\n",
    "    # random index\n",
    "    random_index = random.randint(0, len(mgsm_multilingual['train']) - 1)\n",
    "    first_answer = mgsm_multilingual['train'][random_index]['answer']\n",
    "    answers[key] = first_answer\n",
    "    \n",
    "# Print the answers\n",
    "for lang, answer in answers.items():\n",
    "    print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining Model Generated Output CSV for Future Intuitive Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = [\"question\", \"actual_answer\", \"model_gen_steps\", \"model_gen_answer\",  ]\n",
    "\n",
    "# mgsm_test = GSMDataset(DatasetNames.MGSM, model.tokenizer, instructions='', split='test', config='en')\n",
    "\n",
    "mgsm_generation_df = pd.read_csv('results\\mgsm_en_Qwen2.5-1.5B-Instruct_results.csv')\n",
    "mgsm_generations = mgsm_generation_df['response'].tolist()\n",
    "\n",
    "all_steps = []\n",
    "all_gen_final_ans = []\n",
    "\n",
    "for response in mgsm_generations:\n",
    "    ##ToDo: put this furnshing the steps and answer from raw-answer-csv code to utils\n",
    "    steps = response.split(\"\\n-\")\n",
    "    final_step = steps[-1].split(\".\\r\\n\")[0]\n",
    "    steps.pop()\n",
    "    steps.append( final_step )\n",
    "    furnished_steps = []\n",
    "\n",
    "    i = 1\n",
    "    for _, step in enumerate(steps[1:]):\n",
    "        if step:  # Skip empty parts (if any)\n",
    "            furnished_steps.append(str(i) + \". \" + step )  # Replace with number (1, 2, 3...)\n",
    "            i += 1\n",
    "       \n",
    "\n",
    "    steps_str = \"\".join(furnished_steps)\n",
    "    all_steps.append( steps_str )\n",
    "\n",
    "    gen_final_ans = extract_answer_gsm(response)\n",
    "    all_gen_final_ans.append( gen_final_ans )\n",
    "\n",
    "percentile_list = pd.DataFrame(\n",
    "    {'model_gen_steps': all_steps,\n",
    "     'model_gen_answer': all_gen_final_ans\n",
    "    })\n",
    "\n",
    "percentile_list.to_csv(\"results/analysis_mgsm_en_Qwen2.5-1.5B-Instruct_results.csv\", index = False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# question_list = mgsm_test.dataset['question']\n",
    "# actual_answer = mgsm_test.dataset['answer_number']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNewVenv",
   "language": "python",
   "name": "mynewvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
