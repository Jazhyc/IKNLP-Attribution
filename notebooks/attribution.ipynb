{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: outlines in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: interegular in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (3.1.2)\n",
      "Requirement already satisfied: lark in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.5.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (3.1.0)\n",
      "Requirement already satisfied: diskcache in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (5.6.3)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (2.10.4)\n",
      "Requirement already satisfied: referencing in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.32.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (4.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (4.12.2)\n",
      "Requirement already satisfied: iso3166 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.1.1)\n",
      "Requirement already satisfied: airportsdata in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (20250224)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.6.0+cu124)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.1.26)\n",
      "Requirement already satisfied: genson in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.3.0)\n",
      "Requirement already satisfied: pre-commit>=4.0.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (4.2.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (2.6.9)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (1.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (6.0.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (20.29.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.0->outlines) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.0->outlines) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jinja2->outlines) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (2023.11.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (0.15.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (2023.11.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (3.18.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->outlines) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from sympy==1.13.1->torch->outlines) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from tqdm->outlines) (0.4.6)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from virtualenv>=20.10.0->pre-commit>=4.0.1->outlines) (0.3.9)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from virtualenv>=20.10.0->pre-commit>=4.0.1->outlines) (4.1.0)\n",
      "Collecting context_cite\n",
      "  Downloading context_cite-0.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (1.26.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from context_cite) (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.1.4)\n",
      "Collecting nltk>=3.8.2 (from context_cite)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting spacy (from context_cite)\n",
      "  Using cached spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from context_cite) (4.67.1)\n",
      "Collecting click (from nltk>=3.8.2->context_cite)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from nltk>=3.8.2->context_cite) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from nltk>=3.8.2->context_cite) (2023.10.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (2.31.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->context_cite) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from datasets->context_cite) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from tqdm->context_cite) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from scikit-learn->context_cite) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from scikit-learn->context_cite) (3.2.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->context_cite)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->context_cite)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->context_cite)\n",
      "  Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->context_cite)\n",
      "  Using cached cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->context_cite)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy->context_cite)\n",
      "  Using cached thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->context_cite)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->context_cite)\n",
      "  Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->context_cite)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->context_cite)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->context_cite)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from spacy->context_cite) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from spacy->context_cite) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from spacy->context_cite) (75.8.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->context_cite)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->context_cite) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->context_cite) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->context_cite) (3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->context_cite) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from sympy==1.13.1->torch->context_cite) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers->context_cite) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers->context_cite) (0.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.3.1)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->context_cite)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->context_cite) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->context_cite) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->context_cite) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (2023.11.17)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->context_cite)\n",
      "  Using cached blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->context_cite)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->context_cite)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy->context_cite) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jinja2->spacy->context_cite) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->context_cite)\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (0.1.2)\n",
      "Downloading context_cite-0.0.4-py3-none-any.whl (13 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "Using cached thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, click, catalogue, blis, srsly, smart-open, preshed, nltk, language-data, typer, langcodes, confection, weasel, thinc, spacy, context_cite\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 context_cite-0.0.4 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 nltk-3.9.1 preshed-3.0.9 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install outlines\n",
    "!pip install context_cite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the path to the parent directory to sys\n",
    "import sys, os\n",
    "\n",
    "# If current directory is called 'notebooks', chdir to the parent\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "    \n",
    "sys.path.append('attribution')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from constants import ModelNames, DatasetNames, LANGUAGE_MAPPING\n",
    "from model_utils import Model \n",
    "from dataset_utils import GSMDataset, PaddingCollator, is_correct_gsm, extract_answer_gsm\n",
    "from context_cite import ContextCiter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter specific warning categories\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # For general user warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # For deprecation warnings\n",
    "\n",
    "# Definitions\n",
    "processed_data_path = \"results/analysis_mgsm_en_Qwen2.5-1.5B-Instruct_results.csv\"\n",
    "model_name = ModelNames.QwenInstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Processing Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_dataset():\n",
    "    model = Model(ModelNames.QwenInstruct)\n",
    "    return model, DatasetNames.MGSM\n",
    "\n",
    "class ResponseProcessing():\n",
    "    def __init__(self, model, dataset, config='en', is_cot=True):\n",
    "        self.df_column_names = [\"question\", \"actual_answer\", \"model_gen_steps\", \"model_gen_answer\", 'model_answer_str']\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.is_cot = is_cot\n",
    "    \n",
    "    def convert_dashes_incremental_steps_list(self, steps):\n",
    "        furnished_steps = []\n",
    "\n",
    "        i = 1\n",
    "        for _, step in enumerate(steps[1:]):\n",
    "            if step:  # Skip empty parts (if any)\n",
    "                \n",
    "                # I removed the full stop because contextcite treats the step number itself as a new sentence\n",
    "                furnished_steps.append(str(i) + \" \" + step)  # Replace with number (1, 2, 3...)\n",
    "                i += 1\n",
    "        \n",
    "        return furnished_steps\n",
    "\n",
    "    def convert_dashes_incremental_steps(self, step):\n",
    "\n",
    "        '''\n",
    "        Returns str\n",
    "        '''\n",
    "\n",
    "        furnished_steps = self.convert_dashes_incremental_steps_list(step)\n",
    "\n",
    "        final_str = \"Step-by-Step Answer:\\n\"\n",
    "\n",
    "        # Added a \\n to better separate the steps\n",
    "        final_str += \"\\n\".join(furnished_steps)\n",
    "\n",
    "        return final_str\n",
    "\n",
    "\n",
    "    def process_model_responses_for_analysis(self):\n",
    "        \n",
    "        # Load train for instructions\n",
    "        mgsm_train = GSMDataset(self.dataset, self.model.tokenizer, split='train', config=self.config)\n",
    "        \n",
    "        mgsm_test = GSMDataset(self.dataset, self.model.tokenizer, instructions='', split='test', config=self.config)\n",
    "        \n",
    "        mgsm_generation_df = pd.read_csv('results\\mgsm_en_Qwen2.5-1.5B-Instruct_results.csv')\n",
    "        mgsm_generations = mgsm_generation_df['response'].tolist()\n",
    "        \n",
    "        all_steps = []\n",
    "        all_gen_final_ans = []\n",
    "        all_answer_strings = []  # For storing the last line\n",
    "        \n",
    "        for response in mgsm_generations:\n",
    "            # Split response by newlines\n",
    "            lines = response.strip().split('\\n')\n",
    "            \n",
    "            # Extract the last line as the answer string\n",
    "            answer_string = lines[-1]\n",
    "            all_answer_strings.append(answer_string)\n",
    "            \n",
    "            # Use all lines except the last for steps\n",
    "            remaining_response = '\\n'.join(lines[:-1])\n",
    "            steps = remaining_response.split(\"\\n-\")\n",
    "                \n",
    "            steps_str = self.convert_dashes_incremental_steps(steps)\n",
    "            all_steps.append(steps_str)\n",
    "            \n",
    "            # Extract numerical answer\n",
    "            gen_final_ans = extract_answer_gsm(response)\n",
    "            all_gen_final_ans.append(gen_final_ans)\n",
    "        \n",
    "        # Combine each question with mgsm_train.instructions\n",
    "        instructions = mgsm_train.instructions + '\\n\\n' if self.is_cot else ''\n",
    "        \n",
    "        # Get questions as a list\n",
    "        questions = mgsm_test.dataset['question']\n",
    "        \n",
    "        # Create a list of questions with instructions prepended to each\n",
    "        question_list = [instructions + q for q in questions]\n",
    "        \n",
    "        actual_answer = mgsm_test.dataset['answer_number']\n",
    "        \n",
    "        # Create DataFrame with all columns\n",
    "        percentile_list = pd.DataFrame(\n",
    "            data=zip(question_list, actual_answer, all_steps, all_gen_final_ans, all_answer_strings), \n",
    "            columns=self.df_column_names\n",
    "        )\n",
    "        \n",
    "        percentile_list.to_csv(processed_data_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# This will always be true. \n",
    "# I think you meant to use __name__ == '__main__' but this does not work in Jupyter Notebooks\n",
    "if '__main__':\n",
    "    context_model, dataset = load_model_dataset()\n",
    "    \n",
    "    responseProcessing = ResponseProcessing(context_model, dataset)\n",
    "    responseProcessing.process_model_responses_for_analysis()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    " 1. read from \"analysis_{model_name}\"\n",
    " 2. pass in model_generated_steps and query\n",
    " 3. Check if there answer matches with our answer (I think it might be worthwile to also check wrong answers.)\n",
    " 4. If yes, then use cc.getattribution() to attribution [contextCite](https://github.com/MadryLab/context-cite)\n",
    " 5. Save the np.array to the respective row of the \"analysis_{model_name}\" set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>model_gen_steps</th>\n",
       "      <th>model_gen_answer</th>\n",
       "      <th>model_answer_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>18</td>\n",
       "      <td>Step-by-Step Answer:\\n1  First, calculate the ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>The answer is 18.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The robe requires 2 b...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The answer is 3.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>70000</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The original price of...</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>The answer is 170000.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>540</td>\n",
       "      <td>Step-by-Step Answer:\\n1  James runs 3 sprints ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>- Since he runs 3 times a week, he runs a tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>20</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The total amount of f...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>The answer is 20.&lt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  actual_answer  \\\n",
       "0  Question: Roger has 5 tennis balls. He buys 2 ...             18   \n",
       "1  Question: Roger has 5 tennis balls. He buys 2 ...              3   \n",
       "2  Question: Roger has 5 tennis balls. He buys 2 ...          70000   \n",
       "3  Question: Roger has 5 tennis balls. He buys 2 ...            540   \n",
       "4  Question: Roger has 5 tennis balls. He buys 2 ...             20   \n",
       "\n",
       "                                     model_gen_steps  model_gen_answer  \\\n",
       "0  Step-by-Step Answer:\\n1  First, calculate the ...              18.0   \n",
       "1  Step-by-Step Answer:\\n1  The robe requires 2 b...               3.0   \n",
       "2  Step-by-Step Answer:\\n1  The original price of...          170000.0   \n",
       "3  Step-by-Step Answer:\\n1  James runs 3 sprints ...               3.0   \n",
       "4  Step-by-Step Answer:\\n1  The total amount of f...              20.0   \n",
       "\n",
       "                                    model_answer_str  \n",
       "0                                 The answer is 18.<  \n",
       "1                                  The answer is 3.<  \n",
       "2                             The answer is 170000.<  \n",
       "3  - Since he runs 3 times a week, he runs a tota...  \n",
       "4                                 The answer is 20.<  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_model = Model(ModelNames.QwenInstruct)\n",
    "\n",
    "# Unlike RAG, the context follows the query\n",
    "prompt_template = '{query}\\n{context}'\n",
    "\n",
    "model_responses = pd.read_csv(processed_data_path)\n",
    "model_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_df = pd.DataFrame()\n",
    "\n",
    "# Get length of model_responses\n",
    "len_responses = len(model_responses)\n",
    "\n",
    "# initialize a progress bar\n",
    "pbar = tqdm(total=len_responses)\n",
    "error_counter = 0\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in model_responses.iterrows():\n",
    "    pbar.update(1)\n",
    "    context = row['model_gen_steps']\n",
    "    query = row['question']\n",
    "    answer_string = row['model_answer_str']\n",
    "    \n",
    "    # Abstain from pre-train because it creates a new model each time\n",
    "    # Constructor is needed due to processing during initialization\n",
    "    cc = ContextCiter(context_model.model, context_model.tokenizer, context, query, prompt_template=prompt_template)\n",
    "    \n",
    "    # We want to use precomputed answers\n",
    "    # See https://github.com/MadryLab/context-cite/issues/4\n",
    "    _, prompt = cc._get_prompt_ids(return_prompt=True)\n",
    "    cc._cache[\"output\"] = prompt + answer_string\n",
    "    \n",
    "    # This returns an importance for each line in the context\n",
    "    # The progress bar is annoying\n",
    "    line_importance = cc.get_attributions(as_dataframe=False, verbose=False)\n",
    "    \n",
    "    # Get each line and importance and add to df\n",
    "    lines = context.split('\\n')\n",
    "    \n",
    "    # If number of lines and importance values do not match, raise an error\n",
    "    if len(lines) != len(line_importance):\n",
    "        print(f\"Number of lines ({len(lines)}) and importance values ({len(line_importance)}) do not match in example {index} Skipping...\")\n",
    "        error_counter += 1\n",
    "        continue\n",
    "    \n",
    "    # Create a temporary DataFrame with sample_index to identify which example each line belongs to\n",
    "    temp_df = pd.DataFrame({\n",
    "        'sample_index': index,  # Use the DataFrame index as sample index\n",
    "        'line': lines,\n",
    "        'importance': line_importance\n",
    "    })\n",
    "    \n",
    "    cite_df = pd.concat([cite_df, temp_df], ignore_index=True)\n",
    "    \n",
    "pbar.close()\n",
    "print(f\"Number of errors: {error_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "#TODO Decide on a proper name later, maybe change structure and save more data\n",
    "import json\n",
    "\n",
    "# Create a DataFrame with one row per question\n",
    "compact_df = pd.DataFrame()\n",
    "\n",
    "for sample_index, group in cite_df.groupby('sample_index'):\n",
    "    original_row = model_responses.iloc[sample_index]\n",
    "    \n",
    "    # Create a row dict\n",
    "    row_dict = {\n",
    "        'lines_and_importance': json.dumps([\n",
    "            {'text': row['line'], 'importance': float(row['importance'])} \n",
    "            for _, row in group.iterrows()\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Add row to DataFrame\n",
    "    compact_df = pd.concat([compact_df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "\n",
    "# Save as CSV\n",
    "compact_df.to_csv('results/contextcite_en_QwenInstruct_COT.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
