{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: outlines in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: interegular in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (3.1.2)\n",
      "Requirement already satisfied: lark in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.5.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (3.1.0)\n",
      "Requirement already satisfied: diskcache in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (5.6.3)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (2.10.4)\n",
      "Requirement already satisfied: referencing in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.32.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (4.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from outlines) (4.12.2)\n",
      "Requirement already satisfied: iso3166 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.1.1)\n",
      "Requirement already satisfied: airportsdata in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (20250224)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (2.6.0+cu124)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (0.1.26)\n",
      "Requirement already satisfied: genson in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (1.3.0)\n",
      "Requirement already satisfied: pre-commit>=4.0.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from outlines) (4.2.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (2.6.9)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (1.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (6.0.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pre-commit>=4.0.1->outlines) (20.29.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.0->outlines) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.0->outlines) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jinja2->outlines) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (2023.11.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jsonschema->outlines) (0.15.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests->outlines) (2023.11.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (3.18.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->outlines) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->outlines) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from sympy==1.13.1->torch->outlines) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from tqdm->outlines) (0.4.6)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from virtualenv>=20.10.0->pre-commit>=4.0.1->outlines) (0.3.9)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from virtualenv>=20.10.0->pre-commit>=4.0.1->outlines) (4.1.0)\n",
      "Collecting context_cite\n",
      "  Downloading context_cite-0.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (1.26.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from context_cite) (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (2.1.4)\n",
      "Collecting nltk>=3.8.2 (from context_cite)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting spacy (from context_cite)\n",
      "  Using cached spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from context_cite) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from context_cite) (4.67.1)\n",
      "Collecting click (from nltk>=3.8.2->context_cite)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from nltk>=3.8.2->context_cite) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from nltk>=3.8.2->context_cite) (2023.10.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (2.31.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->context_cite) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from datasets->context_cite) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from datasets->context_cite) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from tqdm->context_cite) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from pandas->context_cite) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from scikit-learn->context_cite) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from scikit-learn->context_cite) (3.2.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->context_cite)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->context_cite)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->context_cite)\n",
      "  Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->context_cite)\n",
      "  Using cached cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->context_cite)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy->context_cite)\n",
      "  Using cached thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->context_cite)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->context_cite)\n",
      "  Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->context_cite)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->context_cite)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->context_cite)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from spacy->context_cite) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from spacy->context_cite) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from spacy->context_cite) (75.8.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->context_cite)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->context_cite) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->context_cite) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from torch->context_cite) (3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from torch->context_cite) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from sympy==1.13.1->torch->context_cite) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers->context_cite) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers->context_cite) (0.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from aiohttp->datasets->context_cite) (1.3.1)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->context_cite)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->context_cite) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->context_cite) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->context_cite) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from requests>=2.19.0->datasets->context_cite) (2023.11.17)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->context_cite)\n",
      "  Using cached blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->context_cite)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->context_cite)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy->context_cite) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.conda\\envs\\transformer\\lib\\site-packages (from jinja2->spacy->context_cite) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->context_cite)\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->context_cite)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->context_cite) (0.1.2)\n",
      "Downloading context_cite-0.0.4-py3-none-any.whl (13 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "Using cached thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, click, catalogue, blis, srsly, smart-open, preshed, nltk, language-data, typer, langcodes, confection, weasel, thinc, spacy, context_cite\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 context_cite-0.0.4 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 nltk-3.9.1 preshed-3.0.9 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install outlines\n",
    "!pip install context_cite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the path to the parent directory to sys\n",
    "import sys, os\n",
    "\n",
    "# If current directory is called 'notebooks', chdir to the parent\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "    \n",
    "sys.path.append('attribution')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from constants import ModelNames, DatasetNames, LANGUAGE_MAPPING\n",
    "from model_utils import Model \n",
    "from dataset_utils import GSMDataset, PaddingCollator, is_correct_gsm, extract_answer_gsm\n",
    "from context_cite import ContextCiter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter specific warning categories\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # For general user warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # For deprecation warnings\n",
    "\n",
    "# Definitions\n",
    "processed_data_path = \"results/analysis_mgsm_en_Qwen2.5-1.5B-Instruct_results.csv\"\n",
    "model_name = ModelNames.QwenInstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Processing Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_dataset():\n",
    "    model = Model(ModelNames.QwenInstruct)\n",
    "    return model, DatasetNames.MGSM\n",
    "\n",
    "class ResponseProcessing():\n",
    "    def __init__(self, model, dataset, config='en', is_cot=True):\n",
    "        self.df_column_names = [\"question\", \"actual_answer\", \"model_gen_steps\", \"model_gen_answer\", 'model_answer_str']\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.is_cot = is_cot\n",
    "    \n",
    "    def convert_dashes_incremental_steps_list(self, steps):\n",
    "        furnished_steps = []\n",
    "\n",
    "        i = 1\n",
    "        for _, step in enumerate(steps[1:]):\n",
    "            if step:  # Skip empty parts (if any)\n",
    "                \n",
    "                # I removed the full stop because contextcite treats the step number itself as a new sentence\n",
    "                furnished_steps.append(str(i) + \" \" + step)  # Replace with number (1, 2, 3...)\n",
    "                i += 1\n",
    "        \n",
    "        return furnished_steps\n",
    "\n",
    "    def convert_dashes_incremental_steps(self, step):\n",
    "\n",
    "        '''\n",
    "        Returns str\n",
    "        '''\n",
    "\n",
    "        furnished_steps = self.convert_dashes_incremental_steps_list(step)\n",
    "\n",
    "        final_str = \"Step-by-Step Answer:\\n\"\n",
    "\n",
    "        # Added a \\n to better separate the steps\n",
    "        final_str += \"\\n\".join(furnished_steps)\n",
    "\n",
    "        return final_str\n",
    "\n",
    "\n",
    "    def process_model_responses_for_analysis(self):\n",
    "        \n",
    "        # Load train for instructions\n",
    "        mgsm_train = GSMDataset(self.dataset, self.model.tokenizer, split='train', config=self.config)\n",
    "        \n",
    "        mgsm_test = GSMDataset(self.dataset, self.model.tokenizer, instructions='', split='test', config=self.config)\n",
    "        \n",
    "        mgsm_generation_df = pd.read_csv('results\\mgsm_en_Qwen2.5-1.5B-Instruct_results.csv')\n",
    "        mgsm_generations = mgsm_generation_df['response'].tolist()\n",
    "        \n",
    "        all_steps = []\n",
    "        all_gen_final_ans = []\n",
    "        all_answer_strings = []  # For storing the last line\n",
    "        \n",
    "        for response in mgsm_generations:\n",
    "            # Split response by newlines\n",
    "            lines = response.strip().split('\\n')\n",
    "            \n",
    "            # Extract the last line as the answer string\n",
    "            answer_string = lines[-1]\n",
    "            all_answer_strings.append(answer_string)\n",
    "            \n",
    "            # Use all lines except the last for steps\n",
    "            remaining_response = '\\n'.join(lines[:-1])\n",
    "            steps = remaining_response.split(\"\\n-\")\n",
    "                \n",
    "            steps_str = self.convert_dashes_incremental_steps(steps)\n",
    "            all_steps.append(steps_str)\n",
    "            \n",
    "            # Extract numerical answer\n",
    "            gen_final_ans = extract_answer_gsm(response)\n",
    "            all_gen_final_ans.append(gen_final_ans)\n",
    "        \n",
    "        # Combine each question with mgsm_train.instructions\n",
    "        instructions = mgsm_train.instructions + '\\n\\n' if self.is_cot else ''\n",
    "        \n",
    "        # Get questions as a list\n",
    "        questions = mgsm_test.dataset['question']\n",
    "        \n",
    "        # Create a list of questions with instructions prepended to each\n",
    "        question_list = [instructions + q for q in questions]\n",
    "        \n",
    "        actual_answer = mgsm_test.dataset['answer_number']\n",
    "        \n",
    "        # Create DataFrame with all columns\n",
    "        percentile_list = pd.DataFrame(\n",
    "            data=zip(question_list, actual_answer, all_steps, all_gen_final_ans, all_answer_strings), \n",
    "            columns=self.df_column_names\n",
    "        )\n",
    "        \n",
    "        percentile_list.to_csv(processed_data_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# This will always be true. \n",
    "# I think you meant to use __name__ == '__main__' but this does not work in Jupyter Notebooks\n",
    "if '__main__':\n",
    "    context_model, dataset = load_model_dataset()\n",
    "    \n",
    "    responseProcessing = ResponseProcessing(context_model, dataset)\n",
    "    responseProcessing.process_model_responses_for_analysis()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    " 1. read from \"analysis_{model_name}\"\n",
    " 2. pass in model_generated_steps and query\n",
    " 3. Check if there answer matches with our answer (I think it might be worthwile to also check wrong answers.)\n",
    " 4. If yes, then use cc.getattribution() to attribution [contextCite](https://github.com/MadryLab/context-cite)\n",
    " 5. Save the np.array to the respective row of the \"analysis_{model_name}\" set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>model_gen_steps</th>\n",
       "      <th>model_gen_answer</th>\n",
       "      <th>model_answer_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>18</td>\n",
       "      <td>Step-by-Step Answer:\\n1  First, calculate the ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>The answer is 18.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The robe requires 2 b...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The answer is 3.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>70000</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The original price of...</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>The answer is 170000.&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>540</td>\n",
       "      <td>Step-by-Step Answer:\\n1  James runs 3 sprints ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>- Since he runs 3 times a week, he runs a tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>20</td>\n",
       "      <td>Step-by-Step Answer:\\n1  The total amount of f...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>The answer is 20.&lt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  actual_answer  \\\n",
       "0  Question: Roger has 5 tennis balls. He buys 2 ...             18   \n",
       "1  Question: Roger has 5 tennis balls. He buys 2 ...              3   \n",
       "2  Question: Roger has 5 tennis balls. He buys 2 ...          70000   \n",
       "3  Question: Roger has 5 tennis balls. He buys 2 ...            540   \n",
       "4  Question: Roger has 5 tennis balls. He buys 2 ...             20   \n",
       "\n",
       "                                     model_gen_steps  model_gen_answer  \\\n",
       "0  Step-by-Step Answer:\\n1  First, calculate the ...              18.0   \n",
       "1  Step-by-Step Answer:\\n1  The robe requires 2 b...               3.0   \n",
       "2  Step-by-Step Answer:\\n1  The original price of...          170000.0   \n",
       "3  Step-by-Step Answer:\\n1  James runs 3 sprints ...               3.0   \n",
       "4  Step-by-Step Answer:\\n1  The total amount of f...              20.0   \n",
       "\n",
       "                                    model_answer_str  \n",
       "0                                 The answer is 18.<  \n",
       "1                                  The answer is 3.<  \n",
       "2                             The answer is 170000.<  \n",
       "3  - Since he runs 3 times a week, he runs a tota...  \n",
       "4                                 The answer is 20.<  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_model = Model(ModelNames.QwenInstruct)\n",
    "\n",
    "# Unlike RAG, the context follows the query\n",
    "prompt_template = '{query}\\n{context}'\n",
    "\n",
    "model_responses = pd.read_csv(processed_data_path)\n",
    "model_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eebb6fdfe214d40a3db1afd16891d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines (4) and importance values (6) do not match in example 6 Skipping...\n",
      "Number of lines (6) and importance values (11) do not match in example 7 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 8 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 10 Skipping...\n",
      "Number of lines (4) and importance values (7) do not match in example 11 Skipping...\n",
      "Number of lines (9) and importance values (11) do not match in example 14 Skipping...\n",
      "Number of lines (4) and importance values (7) do not match in example 34 Skipping...\n",
      "Number of lines (9) and importance values (10) do not match in example 38 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 39 Skipping...\n",
      "Number of lines (3) and importance values (5) do not match in example 40 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 42 Skipping...\n",
      "Number of lines (9) and importance values (11) do not match in example 45 Skipping...\n",
      "Number of lines (3) and importance values (5) do not match in example 48 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 49 Skipping...\n",
      "Number of lines (7) and importance values (9) do not match in example 53 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 61 Skipping...\n",
      "Number of lines (6) and importance values (9) do not match in example 63 Skipping...\n",
      "Number of lines (3) and importance values (7) do not match in example 66 Skipping...\n",
      "Number of lines (4) and importance values (7) do not match in example 67 Skipping...\n",
      "Number of lines (7) and importance values (8) do not match in example 70 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 72 Skipping...\n",
      "Number of lines (4) and importance values (8) do not match in example 77 Skipping...\n",
      "Number of lines (3) and importance values (4) do not match in example 79 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 80 Skipping...\n",
      "Number of lines (9) and importance values (10) do not match in example 84 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 89 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 94 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 95 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 99 Skipping...\n",
      "Number of lines (7) and importance values (8) do not match in example 100 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 102 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 103 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 104 Skipping...\n",
      "Number of lines (10) and importance values (11) do not match in example 108 Skipping...\n",
      "Number of lines (4) and importance values (7) do not match in example 109 Skipping...\n",
      "Number of lines (4) and importance values (7) do not match in example 110 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 111 Skipping...\n",
      "Number of lines (2) and importance values (3) do not match in example 119 Skipping...\n",
      "Number of lines (4) and importance values (8) do not match in example 120 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 121 Skipping...\n",
      "Number of lines (4) and importance values (8) do not match in example 128 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 129 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 130 Skipping...\n",
      "Number of lines (3) and importance values (5) do not match in example 134 Skipping...\n",
      "Number of lines (6) and importance values (9) do not match in example 135 Skipping...\n",
      "Number of lines (4) and importance values (10) do not match in example 136 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 138 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 139 Skipping...\n",
      "Number of lines (6) and importance values (8) do not match in example 140 Skipping...\n",
      "Number of lines (4) and importance values (9) do not match in example 145 Skipping...\n",
      "Number of lines (6) and importance values (13) do not match in example 147 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 149 Skipping...\n",
      "Number of lines (7) and importance values (8) do not match in example 150 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 154 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 156 Skipping...\n",
      "Number of lines (3) and importance values (8) do not match in example 157 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 160 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 163 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 168 Skipping...\n",
      "Number of lines (5) and importance values (6) do not match in example 170 Skipping...\n",
      "Number of lines (6) and importance values (8) do not match in example 174 Skipping...\n",
      "Number of lines (6) and importance values (9) do not match in example 175 Skipping...\n",
      "Number of lines (7) and importance values (10) do not match in example 177 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 178 Skipping...\n",
      "Number of lines (6) and importance values (11) do not match in example 181 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 182 Skipping...\n",
      "Number of lines (5) and importance values (9) do not match in example 183 Skipping...\n",
      "Number of lines (3) and importance values (7) do not match in example 184 Skipping...\n",
      "Number of lines (7) and importance values (8) do not match in example 186 Skipping...\n",
      "Number of lines (3) and importance values (6) do not match in example 190 Skipping...\n",
      "Number of lines (3) and importance values (5) do not match in example 191 Skipping...\n",
      "Number of lines (3) and importance values (5) do not match in example 195 Skipping...\n",
      "Number of lines (5) and importance values (8) do not match in example 199 Skipping...\n",
      "Number of lines (3) and importance values (6) do not match in example 202 Skipping...\n",
      "Number of lines (5) and importance values (8) do not match in example 203 Skipping...\n",
      "Number of lines (5) and importance values (8) do not match in example 204 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 206 Skipping...\n",
      "Number of lines (5) and importance values (9) do not match in example 207 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 209 Skipping...\n",
      "Number of lines (5) and importance values (9) do not match in example 210 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 212 Skipping...\n",
      "Number of lines (8) and importance values (10) do not match in example 214 Skipping...\n",
      "Number of lines (6) and importance values (7) do not match in example 219 Skipping...\n",
      "Number of lines (4) and importance values (5) do not match in example 222 Skipping...\n",
      "Number of lines (6) and importance values (8) do not match in example 228 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 229 Skipping...\n",
      "Number of lines (3) and importance values (6) do not match in example 232 Skipping...\n",
      "Number of lines (4) and importance values (6) do not match in example 233 Skipping...\n",
      "Number of lines (5) and importance values (7) do not match in example 239 Skipping...\n",
      "Number of lines (6) and importance values (9) do not match in example 242 Skipping...\n",
      "Number of lines (4) and importance values (10) do not match in example 243 Skipping...\n",
      "Number of lines (7) and importance values (10) do not match in example 244 Skipping...\n",
      "Number of lines (7) and importance values (9) do not match in example 246 Skipping...\n",
      "Number of errors: 93\n"
     ]
    }
   ],
   "source": [
    "cite_df = pd.DataFrame()\n",
    "\n",
    "# Get length of model_responses\n",
    "len_responses = len(model_responses)\n",
    "\n",
    "# initialize a progress bar\n",
    "pbar = tqdm(total=len_responses)\n",
    "error_counter = 0\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in model_responses.iterrows():\n",
    "    pbar.update(1)\n",
    "    context = row['model_gen_steps']\n",
    "    query = row['question']\n",
    "    answer_string = row['model_answer_str']\n",
    "    \n",
    "    # Abstain from pre-train because it creates a new model each time\n",
    "    # Constructor is needed due to processing during initialization\n",
    "    cc = ContextCiter(context_model.model, context_model.tokenizer, context, query, prompt_template=prompt_template)\n",
    "    \n",
    "    # We want to use precomputed answers\n",
    "    # See https://github.com/MadryLab/context-cite/issues/4\n",
    "    _, prompt = cc._get_prompt_ids(return_prompt=True)\n",
    "    cc._cache[\"output\"] = prompt + answer_string\n",
    "    \n",
    "    # This returns an importance for each line in the context\n",
    "    # The progress bar is annoying\n",
    "    line_importance = cc.get_attributions(as_dataframe=False, verbose=False)\n",
    "    \n",
    "    # Get each line and importance and add to df\n",
    "    lines = context.split('\\n')\n",
    "    \n",
    "    # If number of lines and importance values do not match, raise an error\n",
    "    if len(lines) != len(line_importance):\n",
    "        print(f\"Number of lines ({len(lines)}) and importance values ({len(line_importance)}) do not match in example {index} Skipping...\")\n",
    "        error_counter += 1\n",
    "        continue\n",
    "    \n",
    "    # Create a temporary DataFrame with sample_index to identify which example each line belongs to\n",
    "    temp_df = pd.DataFrame({\n",
    "        'sample_index': index,  # Use the DataFrame index as sample index\n",
    "        'line': lines,\n",
    "        'importance': line_importance\n",
    "    })\n",
    "    \n",
    "    cite_df = pd.concat([cite_df, temp_df], ignore_index=True)\n",
    "    \n",
    "pbar.close()\n",
    "print(f\"Number of errors: {error_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results as JSON\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Create a list to store one dictionary per question\n",
    "result_list = []\n",
    "\n",
    "# Custom JSON encoder to handle NumPy types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "for sample_index, group in cite_df.groupby('sample_index'):\n",
    "    original_row = model_responses.iloc[sample_index]\n",
    "    \n",
    "    # Create a dictionary for this sample\n",
    "    sample_dict = {\n",
    "        'sample_index': sample_index,  # No need to manually convert\n",
    "        'question': original_row['question'],\n",
    "        'actual_answer': int(original_row['actual_answer']),\n",
    "        'model_gen_answer': int(original_row['model_gen_answer']),\n",
    "        'model_answer_str': original_row['model_answer_str'],\n",
    "        'lines_and_importance': [\n",
    "            {'text': row['line'], 'importance': row['importance']} \n",
    "            for _, row in group.iterrows()\n",
    "        ]  # No need to manually convert\n",
    "    }\n",
    "    \n",
    "    # Add this dictionary to our results list\n",
    "    result_list.append(sample_dict)\n",
    "\n",
    "# Save as JSON file with proper formatting and custom encoder\n",
    "with open('results/contextcite_en_QwenInstruct_COT.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_list, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
